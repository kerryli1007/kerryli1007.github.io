<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"kerryli1007.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="目录  业务背景 定义问题 相关包导入 EDA 特征工程 模型建立 预测结果  业务背景Santander 是一家银行，Santander的数据科学家，一直在通过二分类回答如下问题：我们的消费者满意吗？消费者会买这个产品吗？消费者能支付该贷款吗？这样的问题。 本次比赛，参赛选手的任务是在不考虑消费金额的前提下，预测消费者是否会在未来某个时间点做出某笔交易。 定义问题和业务流程目标本赛题，本质上是一">
<meta property="og:type" content="article">
<meta property="og:title" content="Santander Customer Transaction Prediction">
<meta property="og:url" content="https://kerryli1007.github.io/2019/07/07/Santander-Customer-Transaction-Prediction/index.html">
<meta property="og:site_name" content="一颗西蓝花🥦">
<meta property="og:description" content="目录  业务背景 定义问题 相关包导入 EDA 特征工程 模型建立 预测结果  业务背景Santander 是一家银行，Santander的数据科学家，一直在通过二分类回答如下问题：我们的消费者满意吗？消费者会买这个产品吗？消费者能支付该贷款吗？这样的问题。 本次比赛，参赛选手的任务是在不考虑消费金额的前提下，预测消费者是否会在未来某个时间点做出某笔交易。 定义问题和业务流程目标本赛题，本质上是一">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kerryli1007.github.io/output_26_1.png">
<meta property="og:image" content="https://kerryli1007.github.io/">
<meta property="article:published_time" content="2019-07-07T16:00:00.000Z">
<meta property="article:modified_time" content="2023-03-24T10:41:37.525Z">
<meta property="article:author" content="一颗西蓝花🥦">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kerryli1007.github.io/output_26_1.png">


<link rel="canonical" href="https://kerryli1007.github.io/2019/07/07/Santander-Customer-Transaction-Prediction/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://kerryli1007.github.io/2019/07/07/Santander-Customer-Transaction-Prediction/","path":"2019/07/07/Santander-Customer-Transaction-Prediction/","title":"Santander Customer Transaction Prediction"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Santander Customer Transaction Prediction | 一颗西蓝花🥦</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="一颗西蓝花🥦" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">一颗西蓝花🥦</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%9A%E5%8A%A1%E8%83%8C%E6%99%AF"><span class="nav-number">1.</span> <span class="nav-text">业务背景</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89%E9%97%AE%E9%A2%98%E5%92%8C%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">定义问题和业务流程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%AE%E6%A0%87"><span class="nav-number">2.1.</span> <span class="nav-text">目标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%98%E9%87%8F"><span class="nav-number">2.2.</span> <span class="nav-text">变量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%84%E6%8C%87%E6%A0%87"><span class="nav-number">2.3.</span> <span class="nav-text">测评指标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B"><span class="nav-number">2.4.</span> <span class="nav-text">业务流程</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%8C%85%E5%AF%BC%E5%85%A5"><span class="nav-number">3.</span> <span class="nav-text">相关包导入</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#EDA"><span class="nav-number">4.</span> <span class="nav-text">EDA</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9train%E8%BF%9B%E8%A1%8CEDA"><span class="nav-number">4.1.</span> <span class="nav-text">对train进行EDA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9test%E8%BF%9B%E8%A1%8CEDA"><span class="nav-number">4.2.</span> <span class="nav-text">对test进行EDA</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90"><span class="nav-number">4.3.</span> <span class="nav-text">特征相关性分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="nav-number">5.</span> <span class="nav-text">特征工程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">6.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C"><span class="nav-number">7.</span> <span class="nav-text">预测结果</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">一颗西蓝花🥦</p>
  <div class="site-description" itemprop="description">AI/思考/学习</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">108</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://kerryli1007.github.io/2019/07/07/Santander-Customer-Transaction-Prediction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="一颗西蓝花🥦">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一颗西蓝花🥦">
      <meta itemprop="description" content="AI/思考/学习">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Santander Customer Transaction Prediction | 一颗西蓝花🥦">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Santander Customer Transaction Prediction
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-07-08 00:00:00" itemprop="dateCreated datePublished" datetime="2019-07-08T00:00:00+08:00">2019-07-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-24 18:41:37" itemprop="dateModified" datetime="2023-03-24T18:41:37+08:00">2023-03-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E4%BB%A3%E7%A0%81/" itemprop="url" rel="index"><span itemprop="name">代码</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>目录</p>
<ol>
<li>业务背景</li>
<li>定义问题</li>
<li>相关包导入</li>
<li>EDA</li>
<li>特征工程</li>
<li>模型建立</li>
<li>预测结果</li>
</ol>
<h1 id="业务背景"><a href="#业务背景" class="headerlink" title="业务背景"></a>业务背景</h1><p>Santander 是一家银行，Santander的数据科学家，一直在通过二分类回答如下问题：我们的消费者满意吗？消费者会买这个产品吗？消费者能支付该贷款吗？这样的问题。</p>
<p>本次比赛，参赛选手的任务是在不考虑消费金额的前提下，预测消费者是否会在未来某个时间点做出某笔交易。</p>
<h1 id="定义问题和业务流程"><a href="#定义问题和业务流程" class="headerlink" title="定义问题和业务流程"></a>定义问题和业务流程</h1><h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>本赛题，本质上是一个二分类问题，预测消费者是否会在未来某个时间点做出某笔交易。<br>目标是预测<code>target</code> 列的值，比赛的输出为<code>0</code>或<code>1</code>。</p>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>本次比赛，提供给参赛选手的数据集是处理后的脱敏数据集，主要包括三个类别：</p>
<ol>
<li>数字特征变量（合计200个维度）</li>
<li>target列</li>
<li>ID_code</li>
</ol>
<h2 id="测评指标"><a href="#测评指标" class="headerlink" title="测评指标"></a>测评指标</h2><p>比赛的测评标准是<code>AUC</code>。</p>
<h2 id="业务流程"><a href="#业务流程" class="headerlink" title="业务流程"></a>业务流程</h2><ol>
<li><p>首先，探索数据，做好特征工程</p>
</li>
<li><p>接着，选择合适的二分类模型</p>
</li>
<li><p>最后，对模型进行优化</p>
</li>
</ol>
<h1 id="相关包导入"><a href="#相关包导入" class="headerlink" title="相关包导入"></a>相关包导入</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br></pre></td></tr></table></figure>

<h1 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls</span><br></pre></td></tr></table></figure>

<pre><code>1.txt
Santander Customer Transaction Prediction.ipynb
Untitled.ipynb
Untitled1.ipynb
[31mfor_lqy.ipynb[m[m*
idx.npy
[34mimg[m[m/
real_test.npy
[31mtest.csv[m[m*
[31mtrain.csv[m[m*
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">test  = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(train.shape)</span><br><span class="line"><span class="built_in">print</span>(train.head())</span><br></pre></td></tr></table></figure>

<pre><code>(200000, 202)
   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \
0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   
1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   
2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   
3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   
4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   

     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \
0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   
1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   
2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   
3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   
4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   

   var_196  var_197  var_198  var_199  
0   7.8784   8.5635  12.7803  -1.0914  
1   8.1267   8.7889  18.3560   1.9518  
2  -6.5213   8.2675  14.7222   0.3965  
3  -2.9275  10.2922  17.9697  -8.9996  
4   3.9267   9.5031  17.9974  -8.8104  

[5 rows x 202 columns]
</code></pre>
<p>总结：</p>
<ol>
<li>训练集中一共有200000条数据</li>
<li>训练集中一共有202列数据</li>
<li>训练集中的特征一共有200列，特征有200维。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(test.shape)</span><br><span class="line"><span class="built_in">print</span>(test.head())</span><br></pre></td></tr></table></figure>

<pre><code>(200000, 201)
  ID_code    var_0    var_1    var_2   var_3    var_4   var_5   var_6  \
0  test_0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493   
1  test_1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196   
2  test_2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950   
3  test_3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397   
4  test_4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595   

     var_7   var_8   ...     var_190  var_191  var_192  var_193  var_194  \
0  18.2675  2.1337   ...     -2.1556  11.8495  -1.4300   2.4508  13.7112   
1  18.6316 -4.4131   ...     10.6165   8.8349   0.9403  10.1282  15.5765   
2  20.2537  1.5233   ...     -0.7484  10.9935   1.9803   2.1800  12.9813   
3  20.5660  3.3755   ...      9.5702   9.0766   1.6580   3.5813  15.1874   
4  10.6048  2.9890   ...      4.2259   9.1723   1.2835   3.3778  19.5542   

   var_195  var_196  var_197  var_198  var_199  
0   2.4669   4.3654  10.7200  15.4722  -8.7197  
1   0.4773  -1.4852   9.8714  19.1293 -20.9760  
2   2.1281  -7.1086   7.0618  19.8956 -23.1794  
3   3.1656   3.9567   9.2295  13.0168  -4.2108  
4  -0.2860  -5.1612   7.2882  13.9260  -9.1846  

[5 rows x 201 columns]
</code></pre>
<p>总结：</p>
<ol>
<li>训练集中一共有200000条数据</li>
<li>训练集中一共有201列数据，和train相比较，少一列label</li>
<li>训练集中的特征一共有200列，即特征有200维。</li>
</ol>
<h2 id="对train进行EDA"><a href="#对train进行EDA" class="headerlink" title="对train进行EDA"></a>对train进行EDA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 200000 entries, 0 to 199999
Columns: 202 entries, ID_code to var_199
dtypes: float64(200), int64(1), object(1)
memory usage: 308.2+ MB
</code></pre>
<p>总结：</p>
<ol>
<li>train一共有20000条，共占内存308MB</li>
<li>数据类型一共有3类：浮点（200列）、整数（1列），对象（1列）</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.isnull().<span class="built_in">sum</span>().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>




<pre><code>0
</code></pre>
<p>总结：</p>
<ol>
<li>train中不存在NA，说明train中的数据集不需要我们进行任何填充，比较方便</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.describe()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;
</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>target</th>
      <th>var_0</th>
      <th>var_1</th>
      <th>var_2</th>
      <th>var_3</th>
      <th>var_4</th>
      <th>var_5</th>
      <th>var_6</th>
      <th>var_7</th>
      <th>var_8</th>
      <th>...</th>
      <th>var_190</th>
      <th>var_191</th>
      <th>var_192</th>
      <th>var_193</th>
      <th>var_194</th>
      <th>var_195</th>
      <th>var_196</th>
      <th>var_197</th>
      <th>var_198</th>
      <th>var_199</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>...</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
      <td>200000.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.100490</td>
      <td>10.679914</td>
      <td>-1.627622</td>
      <td>10.715192</td>
      <td>6.796529</td>
      <td>11.078333</td>
      <td>-5.065317</td>
      <td>5.408949</td>
      <td>16.545850</td>
      <td>0.284162</td>
      <td>...</td>
      <td>3.234440</td>
      <td>7.438408</td>
      <td>1.927839</td>
      <td>3.331774</td>
      <td>17.993784</td>
      <td>-0.142088</td>
      <td>2.303335</td>
      <td>8.908158</td>
      <td>15.870720</td>
      <td>-3.326537</td>
    </tr>
    <tr>
      <th>std</th>
      <td>0.300653</td>
      <td>3.040051</td>
      <td>4.050044</td>
      <td>2.640894</td>
      <td>2.043319</td>
      <td>1.623150</td>
      <td>7.863267</td>
      <td>0.866607</td>
      <td>3.418076</td>
      <td>3.332634</td>
      <td>...</td>
      <td>4.559922</td>
      <td>3.023272</td>
      <td>1.478423</td>
      <td>3.992030</td>
      <td>3.135162</td>
      <td>1.429372</td>
      <td>5.454369</td>
      <td>0.921625</td>
      <td>3.010945</td>
      <td>10.438015</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.408400</td>
      <td>-15.043400</td>
      <td>2.117100</td>
      <td>-0.040200</td>
      <td>5.074800</td>
      <td>-32.562600</td>
      <td>2.347300</td>
      <td>5.349700</td>
      <td>-10.505500</td>
      <td>...</td>
      <td>-14.093300</td>
      <td>-2.691700</td>
      <td>-3.814500</td>
      <td>-11.783400</td>
      <td>8.694400</td>
      <td>-5.261000</td>
      <td>-14.209600</td>
      <td>5.960600</td>
      <td>6.299300</td>
      <td>-38.852800</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>0.000000</td>
      <td>8.453850</td>
      <td>-4.740025</td>
      <td>8.722475</td>
      <td>5.254075</td>
      <td>9.883175</td>
      <td>-11.200350</td>
      <td>4.767700</td>
      <td>13.943800</td>
      <td>-2.317800</td>
      <td>...</td>
      <td>-0.058825</td>
      <td>5.157400</td>
      <td>0.889775</td>
      <td>0.584600</td>
      <td>15.629800</td>
      <td>-1.170700</td>
      <td>-1.946925</td>
      <td>8.252800</td>
      <td>13.829700</td>
      <td>-11.208475</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.000000</td>
      <td>10.524750</td>
      <td>-1.608050</td>
      <td>10.580000</td>
      <td>6.825000</td>
      <td>11.108250</td>
      <td>-4.833150</td>
      <td>5.385100</td>
      <td>16.456800</td>
      <td>0.393700</td>
      <td>...</td>
      <td>3.203600</td>
      <td>7.347750</td>
      <td>1.901300</td>
      <td>3.396350</td>
      <td>17.957950</td>
      <td>-0.172700</td>
      <td>2.408900</td>
      <td>8.888200</td>
      <td>15.934050</td>
      <td>-2.819550</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>0.000000</td>
      <td>12.758200</td>
      <td>1.358625</td>
      <td>12.516700</td>
      <td>8.324100</td>
      <td>12.261125</td>
      <td>0.924800</td>
      <td>6.003000</td>
      <td>19.102900</td>
      <td>2.937900</td>
      <td>...</td>
      <td>6.406200</td>
      <td>9.512525</td>
      <td>2.949500</td>
      <td>6.205800</td>
      <td>20.396525</td>
      <td>0.829600</td>
      <td>6.556725</td>
      <td>9.593300</td>
      <td>18.064725</td>
      <td>4.836800</td>
    </tr>
    <tr>
      <th>max</th>
      <td>1.000000</td>
      <td>20.315000</td>
      <td>10.376800</td>
      <td>19.353000</td>
      <td>13.188300</td>
      <td>16.671400</td>
      <td>17.251600</td>
      <td>8.447700</td>
      <td>27.691800</td>
      <td>10.151300</td>
      <td>...</td>
      <td>18.440900</td>
      <td>16.716500</td>
      <td>8.402400</td>
      <td>18.281800</td>
      <td>27.928800</td>
      <td>4.272900</td>
      <td>18.321500</td>
      <td>12.000400</td>
      <td>26.079100</td>
      <td>28.500700</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 201 columns</p>
</div>



<p>总结：</p>
<ol>
<li>在train中，target中的<code>1</code>和<code>0</code>的label占比约为：<code>1</code>占比10%；<code>0</code>占比90%。</li>
</ol>
<p>分析：</p>
<ol>
<li>label为<code>1</code>占比10%，label为<code>0</code>占比90%，说明数据存在偏态分布，业务意义是说有过购买行为的1占比10%而已。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df1 = train[train[<span class="string">&#x27;target&#x27;</span>] == <span class="number">0</span>]</span><br><span class="line">df2 = train[train[<span class="string">&#x27;target&#x27;</span>] == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">features = train.columns.values[<span class="number">2</span>:]</span><br></pre></td></tr></table></figure>

<p>分析：</p>
<ol>
<li><p>因为label存在 1 和 0 两种状态，我们可以将label为 0 和label为 1 的observation分开，将其进行<br>对比，看其的features之间，是否存在区别。</p>
</li>
<li><p>为了完成上述目标，我们需要进行 2 步操作</p>
<ul>
<li>将train分为2部分，一部分是label为1的DataFrame，另一部分是label为0的DataFrame。</li>
<li>因为要对比的是features，因此，我们从DataFrame中将features names取出来。</li>
</ul>
</li>
<li><p>完成上述2步操作之后，让我们来看看 df1 和 df2 之间的区别吧。</p>
<ul>
<li>首先我们尝试通过 <code>df.describe()</code> 来探索</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df1.describe())</span><br><span class="line"><span class="built_in">print</span>(df2.describe())</span><br></pre></td></tr></table></figure>

<pre><code>         target          var_0          var_1          var_2          var_3  \
count  179902.0  179902.000000  179902.000000  179902.000000  179902.000000   
mean        0.0      10.626681      -1.695770      10.665876       6.788979   
std         0.0       3.008564       4.024813       2.612961       2.040082   
min         0.0       0.408400     -15.043400       2.117100      -0.040200   
25%         0.0       8.429500      -4.790775       8.698025       5.247625   
50%         0.0      10.478600      -1.682600      10.529000       6.817000   
75%         0.0      12.693075       1.287700      12.463900       8.317875   
max         0.0      20.315000      10.376800      19.353000      13.188300   

               var_4          var_5          var_6          var_7  \
count  179902.000000  179902.000000  179902.000000  179902.000000   
mean       11.072412      -5.146736       5.389620      16.549306   
std         1.620103       7.827522       0.857983       3.417700   
min         5.074800     -32.562600       2.347300       5.349700   
25%         9.880600     -11.260950       4.756425      13.950125   
50%        11.104400      -4.917700       5.364400      16.460850   
75%        12.253100       0.844600       5.981300      19.108800   
max        16.671400      17.251600       8.447700      27.691800   

               var_8      ...              var_190        var_191  \
count  179902.000000      ...        179902.000000  179902.000000   
mean        0.262347      ...             3.149130       7.390800   
std         3.331105      ...             4.522568       2.997847   
min       -10.505500      ...           -14.093300      -2.691700   
25%        -2.342575      ...            -0.111850       5.130825   
50%         0.371400      ...             3.117250       7.297000   
75%         2.919300      ...             6.296375       9.461300   
max        10.151300      ...            18.440900      16.716500   

             var_192        var_193        var_194        var_195  \
count  179902.000000  179902.000000  179902.000000  179902.000000   
mean        1.949017       3.355403      18.017716      -0.155601   
std         1.476456       3.982819       3.127715       1.422275   
min        -3.814500     -10.845500       8.694400      -5.261000   
25%         0.915300       0.612400      15.656700      -1.178700   
50%         1.928950       3.417550      17.982400      -0.184200   
75%         2.969800       6.217075      20.417500       0.813000   
max         8.402400      18.281800      27.928800       4.272900   

             var_196        var_197        var_198        var_199  
count  179902.000000  179902.000000  179902.000000  179902.000000  
mean        2.260297       8.919032      15.924058      -3.415273  
std         5.441118       0.917467       2.978539      10.434525  
min       -14.209600       5.960600       6.299300     -38.852800  
25%        -1.987375       8.262100      13.896000     -11.312025  
50%         2.359700       8.897700      15.988500      -2.914000  
75%         6.513425       9.601500      18.095400       4.741400  
max        18.321500      12.000400      26.079100      28.500700  

[8 rows x 201 columns]
        target         var_0         var_1         var_2         var_3  \
count  20098.0  20098.000000  20098.000000  20098.000000  20098.000000   
mean       1.0     11.156418     -1.017613     11.156633      6.864113   
std        0.0      3.270293      4.220638      2.841075      2.070898   
min        1.0      0.452800    -14.037000      2.946200      0.374000   
25%        1.0      8.695875     -4.203475      8.961125      5.314300   
50%        1.0     11.001350     -0.992650     11.096700      6.900200   
75%        1.0     13.343700      2.001375     13.047025      8.384725   
max        1.0     19.458300      9.029800     18.294100     12.706900   

              var_4         var_5         var_6         var_7         var_8  \
count  20098.000000  20098.000000  20098.000000  20098.000000  20098.000000   
mean      11.131337     -4.336522      5.581966     16.514917      0.479432   
std        1.649266      8.140281      0.922442      3.421365      3.340028   
min        5.876200    -28.246100      2.496000      7.302400     -9.839100   
25%        9.911250    -10.615800      4.882250     13.880125     -2.107425   
50%       11.156300     -4.101950      5.601800     16.412700      0.579600   
75%       12.328450      1.619950      6.218550     19.044250      3.127825   
max       15.692500     16.423600      8.285200     27.039800      9.033000   

           ...            var_190       var_191       var_192       var_193  \
count      ...       20098.000000  20098.000000  20098.000000  20098.000000   
mean       ...           3.998064      7.864560      1.738266      3.120260   
std        ...           4.814830      3.210779      1.482537      4.067550   
min        ...         -11.906900     -2.343000     -3.317700    -11.783400   
25%        ...           0.466025      5.418550      0.669775      0.343000   
50%        ...           4.017500      7.846550      1.667500      3.176700   
75%        ...           7.407925     10.018400      2.749050      6.063825   
max        ...          16.746100     16.520500      7.647600     17.150400   

            var_194       var_195       var_196       var_197       var_198  \
count  20098.000000  20098.000000  20098.000000  20098.000000  20098.000000   
mean      17.779568     -0.021130      2.688583      8.810815     15.393283   
std        3.193153      1.485975      5.556892      0.952554      3.248123   
min       10.120700     -5.018500    -14.020400      6.119000      6.558700   
25%       15.420025     -1.103950     -1.572225      8.155250     13.232500   
50%       17.730500     -0.057350      2.831050      8.795400     15.427950   
75%       20.201800      0.986450      6.956250      9.517000     17.776425   
max       27.295300      4.088100     17.161400     11.706900     25.857100   

            var_199  
count  20098.000000  
mean      -2.532243  
std       10.435910  
min      -38.852800  
25%      -10.285050  
50%       -1.971850  
75%        5.701475  
max       24.564600  

[8 rows x 201 columns]
</code></pre>
<p>总结：</p>
<ol>
<li>很明显，数据描述，无法可视化出<code>df1</code>和<code>df2</code>的区别。</li>
<li>换个思路，可以采取数据可视化，来查看两个df之间的feature分布形状，是否存在差异</li>
<li>思考，在绘制两个df的对比图的时候，是选用条形图，还是折线图。<ul>
<li>折线图是表现随着时间的变化，数据的变化</li>
<li>条形图是表现数量</li>
<li>因此，对比时候，选择条形图</li>
</ul>
</li>
</ol>
<h2 id="对test进行EDA"><a href="#对test进行EDA" class="headerlink" title="对test进行EDA"></a>对test进行EDA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 200000 entries, 0 to 199999
Columns: 201 entries, ID_code to var_199
dtypes: float64(200), object(1)
memory usage: 306.7+ MB
</code></pre>
<p>总结：</p>
<ol>
<li>train一共有20,000条，共占内存308MB</li>
<li>数据类型一共有3类：浮点（200列），对象（1列）,其中<code>ID_code</code>是的字段信息是对象。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize = (<span class="number">16</span>,<span class="number">6</span>))</span><br><span class="line">features = train.columns.values[<span class="number">2</span>:]</span><br><span class="line">plt.title(<span class="string">&#x27;Distribution of mean values per row comparing df1 and df2&#x27;</span>)</span><br><span class="line">sns.distplot(df1[features].mean(axis = <span class="number">1</span>), color =<span class="string">&#x27;green&#x27;</span>, kde = <span class="literal">True</span>, bins =<span class="number">200</span>, </span><br><span class="line">             label = <span class="string">&#x27;df1&#x27;</span>)</span><br><span class="line">sns.distplot(df2[features].mean(axis = <span class="number">1</span>), color =<span class="string">&#x27;blue&#x27;</span>, kde = <span class="literal">True</span>, bins =<span class="number">200</span>, </span><br><span class="line">             label = <span class="string">&#x27;df2&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval
</code></pre>
<p><img src="/output_26_1.png" alt="png"></p>
<p> 总结：</p>
<ol>
<li>从上图可以看出，相比df1，df2的数值分布更加右倾</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.nunique().sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>target          2
var_68        451
var_91       7962
var_108      8525
var_103      9376
var_12       9561
var_148     10608
var_161     11071
var_71      13527
var_25      14853
var_43      15188
var_125     16059
var_166     17902
var_169     18242
var_133     19236
var_15      19810
var_131     21464
var_23      24913
var_34      25164
var_93      26708
var_95      29387
var_42      31592
var_50      32308
var_126     32411
var_98      33266
var_53      33460
var_57      35545
var_28      35859
var_130     36638
var_59      37744
            ...  
var_118    143667
var_19     144180
var_83     144281
var_137    144397
var_158    144556
var_54     144776
var_82     144829
var_184    145184
var_178    145235
var_30     145977
var_102    146237
var_96     148099
var_149    148504
var_182    149195
var_199    149430
var_100    150727
var_48     152039
var_70     153193
var_47     154781
var_160    156274
var_136    156615
var_187    157031
var_90     157210
var_120    158269
var_97     158739
var_61     159369
var_74     161058
var_117    164469
var_45     169968
ID_code    200000
Length: 202, dtype: int64
</code></pre>
<p>总结：</p>
<ol>
<li>我们发现变量<code>var_68</code>这个变量很特殊，只有451个。</li>
</ol>
<h2 id="特征相关性分析"><a href="#特征相关性分析" class="headerlink" title="特征相关性分析"></a>特征相关性分析</h2><p>接着，我们看一下每个特征和<code>label</code>的相关度，便于我们进一步了解特征的重要程度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.corr()[<span class="string">&#x27;target&#x27;</span>].<span class="built_in">abs</span>().sort_values()</span><br></pre></td></tr></table></figure>




<pre><code>var_185    0.000053
var_27     0.000582
var_30     0.000638
var_17     0.000864
var_38     0.000970
var_41     0.001298
var_126    0.001393
var_103    0.001395
var_10     0.002213
var_100    0.002215
var_117    0.002591
var_7      0.003025
var_96     0.003037
var_136    0.003554
var_158    0.003817
var_98     0.004074
var_39     0.004090
var_161    0.004168
var_124    0.004218
var_29     0.004682
var_160    0.005135
var_183    0.005467
var_46     0.005690
var_129    0.005880
var_60     0.006265
var_14     0.006332
var_73     0.006460
var_153    0.007103
var_182    0.007198
var_61     0.007407
             ...   
var_40     0.049530
var_109    0.049926
var_179    0.050002
var_115    0.050174
var_1      0.050343
var_0      0.052390
var_34     0.052692
var_198    0.053000
var_133    0.054548
var_148    0.055011
var_13     0.055156
var_165    0.055734
var_2      0.055870
var_190    0.055973
var_80     0.057609
var_166    0.057773
var_99     0.058367
var_21     0.058483
var_22     0.060558
var_174    0.061669
var_76     0.061917
var_26     0.062422
var_53     0.063399
var_146    0.063644
var_110    0.064275
var_6      0.066731
var_12     0.069489
var_139    0.074080
var_81     0.080917
target     1.000000
Name: target, Length: 201, dtype: float64
</code></pre>
<p>总结：</p>
<ol>
<li>相关性最小的是变量<code>var_185</code>,其相关性是0.000053</li>
<li>相关性最大的是变量<code>var_81</code>,其相关性是0.080917</li>
</ol>
<h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><p>将相关性不高的特征去掉</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train_x = train.drop([<span class="string">&#x27;ID_code&#x27;</span>,<span class="string">&#x27;var_185&#x27;</span>,<span class="string">&#x27;var_27&#x27;</span>,<span class="string">&#x27;var_30&#x27;</span>,<span class="string">&#x27;var_17&#x27;</span>,<span class="string">&#x27;var_38&#x27;</span>,</span><br><span class="line">                     <span class="string">&#x27;var_41&#x27;</span>,<span class="string">&#x27;var_126&#x27;</span>,<span class="string">&#x27;var_103&#x27;</span>,<span class="string">&#x27;target&#x27;</span>],axis = <span class="number">1</span>)</span><br><span class="line">test_x = test.drop([<span class="string">&#x27;ID_code&#x27;</span>,<span class="string">&#x27;var_185&#x27;</span>,<span class="string">&#x27;var_27&#x27;</span>,<span class="string">&#x27;var_30&#x27;</span>,<span class="string">&#x27;var_17&#x27;</span>,<span class="string">&#x27;var_38&#x27;</span>,</span><br><span class="line">                     <span class="string">&#x27;var_41&#x27;</span>,<span class="string">&#x27;var_126&#x27;</span>,<span class="string">&#x27;var_103&#x27;</span>],axis = <span class="number">1</span>)</span><br><span class="line">train_y = train[<span class="string">&#x27;target&#x27;</span>]</span><br></pre></td></tr></table></figure>

<h1 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line">lgb_model = lgb.LGBMClassifier(</span><br><span class="line">    boosting_type=<span class="string">&quot;gbdt&quot;</span>, num_leaves=<span class="number">30</span>, reg_alpha=<span class="number">0</span>, reg_lambda=<span class="number">0.</span>,</span><br><span class="line">    max_depth=-<span class="number">1</span>, n_estimators=<span class="number">2500</span>, objective=<span class="string">&#x27;binary&#x27;</span>,metric= <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">    subsample=<span class="number">0.9</span>, colsample_bytree=<span class="number">0.7</span>, subsample_freq=<span class="number">1</span>,</span><br><span class="line">    learning_rate=<span class="number">0.1</span>, random_state=<span class="number">2018</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">#模型训练</span></span><br><span class="line">X_train, X_test, Y_train, Y_test= train_test_split(train_x, train_y,</span><br><span class="line">                                                   test_size=<span class="number">0.2</span>,random_state=<span class="number">0</span>)</span><br><span class="line">lgb_model.fit(X_train, Y_train,eval_set=[(X_train, Y_train),(X_test, Y_test)], </span><br><span class="line">              early_stopping_rounds=<span class="number">100</span>,verbose=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#模型预测1，预测概率</span></span><br></pre></td></tr></table></figure>

<pre><code>[1]	training&#39;s auc: 0.667923	valid_1&#39;s auc: 0.655931
Training until validation scores don&#39;t improve for 100 rounds.
[2]	training&#39;s auc: 0.700055	valid_1&#39;s auc: 0.680243
[3]	training&#39;s auc: 0.722262	valid_1&#39;s auc: 0.701922
[4]	training&#39;s auc: 0.743563	valid_1&#39;s auc: 0.720751
[5]	training&#39;s auc: 0.751368	valid_1&#39;s auc: 0.729526
[6]	training&#39;s auc: 0.768131	valid_1&#39;s auc: 0.743963
[7]	training&#39;s auc: 0.779217	valid_1&#39;s auc: 0.753297
[8]	training&#39;s auc: 0.784602	valid_1&#39;s auc: 0.758284
[9]	training&#39;s auc: 0.789159	valid_1&#39;s auc: 0.76161
[10]	training&#39;s auc: 0.794783	valid_1&#39;s auc: 0.766658
[11]	training&#39;s auc: 0.801477	valid_1&#39;s auc: 0.772359
[12]	training&#39;s auc: 0.807318	valid_1&#39;s auc: 0.777839
[13]	training&#39;s auc: 0.811805	valid_1&#39;s auc: 0.782093
[14]	training&#39;s auc: 0.81645	valid_1&#39;s auc: 0.787162
[15]	training&#39;s auc: 0.819791	valid_1&#39;s auc: 0.789338
[16]	training&#39;s auc: 0.823462	valid_1&#39;s auc: 0.792981
[17]	training&#39;s auc: 0.826694	valid_1&#39;s auc: 0.795185
[18]	training&#39;s auc: 0.82971	valid_1&#39;s auc: 0.796471
[19]	training&#39;s auc: 0.833108	valid_1&#39;s auc: 0.798868
[20]	training&#39;s auc: 0.83615	valid_1&#39;s auc: 0.801453
[21]	training&#39;s auc: 0.839248	valid_1&#39;s auc: 0.803501
[22]	training&#39;s auc: 0.842092	valid_1&#39;s auc: 0.80569
[23]	training&#39;s auc: 0.844132	valid_1&#39;s auc: 0.806768
[24]	training&#39;s auc: 0.846149	valid_1&#39;s auc: 0.80737
[25]	training&#39;s auc: 0.848712	valid_1&#39;s auc: 0.809792
[26]	training&#39;s auc: 0.851083	valid_1&#39;s auc: 0.812077
[27]	training&#39;s auc: 0.853398	valid_1&#39;s auc: 0.813731
[28]	training&#39;s auc: 0.855374	valid_1&#39;s auc: 0.815894
[29]	training&#39;s auc: 0.857196	valid_1&#39;s auc: 0.817552
[30]	training&#39;s auc: 0.859382	valid_1&#39;s auc: 0.819105
[31]	training&#39;s auc: 0.861022	valid_1&#39;s auc: 0.820354
[32]	training&#39;s auc: 0.863121	valid_1&#39;s auc: 0.822139
[33]	training&#39;s auc: 0.86467	valid_1&#39;s auc: 0.823563
[34]	training&#39;s auc: 0.866346	valid_1&#39;s auc: 0.824579
[35]	training&#39;s auc: 0.868063	valid_1&#39;s auc: 0.825775
[36]	training&#39;s auc: 0.869377	valid_1&#39;s auc: 0.826982
[37]	training&#39;s auc: 0.871411	valid_1&#39;s auc: 0.828624
[38]	training&#39;s auc: 0.872762	valid_1&#39;s auc: 0.829544
[39]	training&#39;s auc: 0.873784	valid_1&#39;s auc: 0.83038
[40]	training&#39;s auc: 0.875417	valid_1&#39;s auc: 0.831754
[41]	training&#39;s auc: 0.876941	valid_1&#39;s auc: 0.833442
[42]	training&#39;s auc: 0.87793	valid_1&#39;s auc: 0.834047
[43]	training&#39;s auc: 0.879483	valid_1&#39;s auc: 0.835804
[44]	training&#39;s auc: 0.880732	valid_1&#39;s auc: 0.836579
[45]	training&#39;s auc: 0.881468	valid_1&#39;s auc: 0.837113
[46]	training&#39;s auc: 0.882515	valid_1&#39;s auc: 0.837551
[47]	training&#39;s auc: 0.88399	valid_1&#39;s auc: 0.838588
[48]	training&#39;s auc: 0.885351	valid_1&#39;s auc: 0.839587
[49]	training&#39;s auc: 0.886336	valid_1&#39;s auc: 0.84068
[50]	training&#39;s auc: 0.887447	valid_1&#39;s auc: 0.84144
[51]	training&#39;s auc: 0.888682	valid_1&#39;s auc: 0.842055
[52]	training&#39;s auc: 0.88981	valid_1&#39;s auc: 0.842932
[53]	training&#39;s auc: 0.890777	valid_1&#39;s auc: 0.843625
[54]	training&#39;s auc: 0.891624	valid_1&#39;s auc: 0.844176
[55]	training&#39;s auc: 0.892606	valid_1&#39;s auc: 0.845134
[56]	training&#39;s auc: 0.893319	valid_1&#39;s auc: 0.845769
[57]	training&#39;s auc: 0.894307	valid_1&#39;s auc: 0.846424
[58]	training&#39;s auc: 0.89513	valid_1&#39;s auc: 0.847148
[59]	training&#39;s auc: 0.895764	valid_1&#39;s auc: 0.847475
[60]	training&#39;s auc: 0.896639	valid_1&#39;s auc: 0.848196
[61]	training&#39;s auc: 0.897383	valid_1&#39;s auc: 0.848451
[62]	training&#39;s auc: 0.89811	valid_1&#39;s auc: 0.848789
[63]	training&#39;s auc: 0.89923	valid_1&#39;s auc: 0.849696
[64]	training&#39;s auc: 0.899986	valid_1&#39;s auc: 0.850134
[65]	training&#39;s auc: 0.900806	valid_1&#39;s auc: 0.850742
[66]	training&#39;s auc: 0.901493	valid_1&#39;s auc: 0.851172
[67]	training&#39;s auc: 0.902277	valid_1&#39;s auc: 0.851724
[68]	training&#39;s auc: 0.903075	valid_1&#39;s auc: 0.852341
[69]	training&#39;s auc: 0.903698	valid_1&#39;s auc: 0.852531
[70]	training&#39;s auc: 0.904426	valid_1&#39;s auc: 0.85275
[71]	training&#39;s auc: 0.905095	valid_1&#39;s auc: 0.853281
[72]	training&#39;s auc: 0.905727	valid_1&#39;s auc: 0.853776
[73]	training&#39;s auc: 0.906332	valid_1&#39;s auc: 0.854497
[74]	training&#39;s auc: 0.907136	valid_1&#39;s auc: 0.855012
[75]	training&#39;s auc: 0.907866	valid_1&#39;s auc: 0.855485
[76]	training&#39;s auc: 0.908379	valid_1&#39;s auc: 0.855842
[77]	training&#39;s auc: 0.909143	valid_1&#39;s auc: 0.856445
[78]	training&#39;s auc: 0.909636	valid_1&#39;s auc: 0.856614
[79]	training&#39;s auc: 0.910341	valid_1&#39;s auc: 0.857097
[80]	training&#39;s auc: 0.910983	valid_1&#39;s auc: 0.857316
[81]	training&#39;s auc: 0.91158	valid_1&#39;s auc: 0.85756
[82]	training&#39;s auc: 0.912069	valid_1&#39;s auc: 0.85798
[83]	training&#39;s auc: 0.91265	valid_1&#39;s auc: 0.858298
[84]	training&#39;s auc: 0.913179	valid_1&#39;s auc: 0.858778
[85]	training&#39;s auc: 0.913738	valid_1&#39;s auc: 0.859048
[86]	training&#39;s auc: 0.914382	valid_1&#39;s auc: 0.859458
[87]	training&#39;s auc: 0.915016	valid_1&#39;s auc: 0.859835
[88]	training&#39;s auc: 0.91557	valid_1&#39;s auc: 0.860216
[89]	training&#39;s auc: 0.91609	valid_1&#39;s auc: 0.86046
[90]	training&#39;s auc: 0.916475	valid_1&#39;s auc: 0.860606
[91]	training&#39;s auc: 0.917059	valid_1&#39;s auc: 0.860715
[92]	training&#39;s auc: 0.917494	valid_1&#39;s auc: 0.860964
[93]	training&#39;s auc: 0.918101	valid_1&#39;s auc: 0.861317
[94]	training&#39;s auc: 0.918533	valid_1&#39;s auc: 0.861529
[95]	training&#39;s auc: 0.919024	valid_1&#39;s auc: 0.861838
[96]	training&#39;s auc: 0.919421	valid_1&#39;s auc: 0.861849
[97]	training&#39;s auc: 0.92	valid_1&#39;s auc: 0.86223
[98]	training&#39;s auc: 0.920422	valid_1&#39;s auc: 0.862529
[99]	training&#39;s auc: 0.92079	valid_1&#39;s auc: 0.86287
[100]	training&#39;s auc: 0.921205	valid_1&#39;s auc: 0.863401
[101]	training&#39;s auc: 0.921648	valid_1&#39;s auc: 0.863488
[102]	training&#39;s auc: 0.922054	valid_1&#39;s auc: 0.863746
[103]	training&#39;s auc: 0.922528	valid_1&#39;s auc: 0.864015
[104]	training&#39;s auc: 0.923002	valid_1&#39;s auc: 0.864111
[105]	training&#39;s auc: 0.923607	valid_1&#39;s auc: 0.864253
[106]	training&#39;s auc: 0.923994	valid_1&#39;s auc: 0.864612
[107]	training&#39;s auc: 0.924451	valid_1&#39;s auc: 0.864899
[108]	training&#39;s auc: 0.92489	valid_1&#39;s auc: 0.865223
[109]	training&#39;s auc: 0.925322	valid_1&#39;s auc: 0.865532
[110]	training&#39;s auc: 0.925653	valid_1&#39;s auc: 0.86569
[111]	training&#39;s auc: 0.926114	valid_1&#39;s auc: 0.866027
[112]	training&#39;s auc: 0.926465	valid_1&#39;s auc: 0.866164
[113]	training&#39;s auc: 0.926848	valid_1&#39;s auc: 0.866304
[114]	training&#39;s auc: 0.927238	valid_1&#39;s auc: 0.866739
[115]	training&#39;s auc: 0.927693	valid_1&#39;s auc: 0.866936
[116]	training&#39;s auc: 0.928001	valid_1&#39;s auc: 0.867235
[117]	training&#39;s auc: 0.928342	valid_1&#39;s auc: 0.867293
[118]	training&#39;s auc: 0.92878	valid_1&#39;s auc: 0.867637
[119]	training&#39;s auc: 0.929076	valid_1&#39;s auc: 0.867797
[120]	training&#39;s auc: 0.929514	valid_1&#39;s auc: 0.868185
[121]	training&#39;s auc: 0.929865	valid_1&#39;s auc: 0.86847
[122]	training&#39;s auc: 0.930259	valid_1&#39;s auc: 0.868747
[123]	training&#39;s auc: 0.930635	valid_1&#39;s auc: 0.868887
[124]	training&#39;s auc: 0.930983	valid_1&#39;s auc: 0.86915
[125]	training&#39;s auc: 0.931312	valid_1&#39;s auc: 0.869264
[126]	training&#39;s auc: 0.931676	valid_1&#39;s auc: 0.86952
[127]	training&#39;s auc: 0.932038	valid_1&#39;s auc: 0.869743
[128]	training&#39;s auc: 0.932308	valid_1&#39;s auc: 0.869901
[129]	training&#39;s auc: 0.932639	valid_1&#39;s auc: 0.869992
[130]	training&#39;s auc: 0.933044	valid_1&#39;s auc: 0.870122
[131]	training&#39;s auc: 0.933414	valid_1&#39;s auc: 0.870339
[132]	training&#39;s auc: 0.933741	valid_1&#39;s auc: 0.870475
[133]	training&#39;s auc: 0.934076	valid_1&#39;s auc: 0.870675
[134]	training&#39;s auc: 0.934437	valid_1&#39;s auc: 0.870797
[135]	training&#39;s auc: 0.934679	valid_1&#39;s auc: 0.870938
[136]	training&#39;s auc: 0.934991	valid_1&#39;s auc: 0.871092
[137]	training&#39;s auc: 0.935274	valid_1&#39;s auc: 0.871226
[138]	training&#39;s auc: 0.935532	valid_1&#39;s auc: 0.871319
[139]	training&#39;s auc: 0.935795	valid_1&#39;s auc: 0.871567
[140]	training&#39;s auc: 0.936095	valid_1&#39;s auc: 0.87169
[141]	training&#39;s auc: 0.936345	valid_1&#39;s auc: 0.871932
[142]	training&#39;s auc: 0.936658	valid_1&#39;s auc: 0.872025
[143]	training&#39;s auc: 0.936884	valid_1&#39;s auc: 0.872195
[144]	training&#39;s auc: 0.93716	valid_1&#39;s auc: 0.872421
[145]	training&#39;s auc: 0.937488	valid_1&#39;s auc: 0.872622
[146]	training&#39;s auc: 0.93778	valid_1&#39;s auc: 0.872901
[147]	training&#39;s auc: 0.938049	valid_1&#39;s auc: 0.872903
[148]	training&#39;s auc: 0.938327	valid_1&#39;s auc: 0.873127
[149]	training&#39;s auc: 0.93865	valid_1&#39;s auc: 0.873202
[150]	training&#39;s auc: 0.938961	valid_1&#39;s auc: 0.87339
[151]	training&#39;s auc: 0.939213	valid_1&#39;s auc: 0.873437
[152]	training&#39;s auc: 0.939494	valid_1&#39;s auc: 0.873555
[153]	training&#39;s auc: 0.939811	valid_1&#39;s auc: 0.87369
[154]	training&#39;s auc: 0.940063	valid_1&#39;s auc: 0.873916
[155]	training&#39;s auc: 0.940306	valid_1&#39;s auc: 0.874046
[156]	training&#39;s auc: 0.940528	valid_1&#39;s auc: 0.874203
[157]	training&#39;s auc: 0.940783	valid_1&#39;s auc: 0.87434
[158]	training&#39;s auc: 0.94106	valid_1&#39;s auc: 0.874377
[159]	training&#39;s auc: 0.941307	valid_1&#39;s auc: 0.874496
[160]	training&#39;s auc: 0.941575	valid_1&#39;s auc: 0.874582
[161]	training&#39;s auc: 0.94184	valid_1&#39;s auc: 0.874549
[162]	training&#39;s auc: 0.942124	valid_1&#39;s auc: 0.874701
[163]	training&#39;s auc: 0.942302	valid_1&#39;s auc: 0.874737
[164]	training&#39;s auc: 0.942545	valid_1&#39;s auc: 0.874938
[165]	training&#39;s auc: 0.942806	valid_1&#39;s auc: 0.875032
[166]	training&#39;s auc: 0.94308	valid_1&#39;s auc: 0.875288
[167]	training&#39;s auc: 0.943333	valid_1&#39;s auc: 0.875378
[168]	training&#39;s auc: 0.943651	valid_1&#39;s auc: 0.875559
[169]	training&#39;s auc: 0.943905	valid_1&#39;s auc: 0.875624
[170]	training&#39;s auc: 0.944104	valid_1&#39;s auc: 0.875843
[171]	training&#39;s auc: 0.944416	valid_1&#39;s auc: 0.875988
[172]	training&#39;s auc: 0.94464	valid_1&#39;s auc: 0.876148
[173]	training&#39;s auc: 0.944872	valid_1&#39;s auc: 0.876228
[174]	training&#39;s auc: 0.94507	valid_1&#39;s auc: 0.876372
[175]	training&#39;s auc: 0.945335	valid_1&#39;s auc: 0.876321
[176]	training&#39;s auc: 0.945594	valid_1&#39;s auc: 0.876503
[177]	training&#39;s auc: 0.945798	valid_1&#39;s auc: 0.876516
[178]	training&#39;s auc: 0.946029	valid_1&#39;s auc: 0.876518
[179]	training&#39;s auc: 0.946279	valid_1&#39;s auc: 0.876514
[180]	training&#39;s auc: 0.946473	valid_1&#39;s auc: 0.876597
[181]	training&#39;s auc: 0.946676	valid_1&#39;s auc: 0.876673
[182]	training&#39;s auc: 0.94693	valid_1&#39;s auc: 0.876814
[183]	training&#39;s auc: 0.947169	valid_1&#39;s auc: 0.876774
[184]	training&#39;s auc: 0.947436	valid_1&#39;s auc: 0.877079
[185]	training&#39;s auc: 0.947611	valid_1&#39;s auc: 0.877106
[186]	training&#39;s auc: 0.947827	valid_1&#39;s auc: 0.877176
[187]	training&#39;s auc: 0.948019	valid_1&#39;s auc: 0.877244
[188]	training&#39;s auc: 0.948306	valid_1&#39;s auc: 0.877291
[189]	training&#39;s auc: 0.948537	valid_1&#39;s auc: 0.877349
[190]	training&#39;s auc: 0.94872	valid_1&#39;s auc: 0.877488
[191]	training&#39;s auc: 0.948972	valid_1&#39;s auc: 0.877548
[192]	training&#39;s auc: 0.949102	valid_1&#39;s auc: 0.877549
[193]	training&#39;s auc: 0.949269	valid_1&#39;s auc: 0.877622
[194]	training&#39;s auc: 0.949451	valid_1&#39;s auc: 0.877733
[195]	training&#39;s auc: 0.94972	valid_1&#39;s auc: 0.877779
[196]	training&#39;s auc: 0.949953	valid_1&#39;s auc: 0.877877
[197]	training&#39;s auc: 0.950191	valid_1&#39;s auc: 0.878049
[198]	training&#39;s auc: 0.950392	valid_1&#39;s auc: 0.878129
[199]	training&#39;s auc: 0.950623	valid_1&#39;s auc: 0.87826
[200]	training&#39;s auc: 0.950832	valid_1&#39;s auc: 0.878285
[201]	training&#39;s auc: 0.951092	valid_1&#39;s auc: 0.878367
[202]	training&#39;s auc: 0.95132	valid_1&#39;s auc: 0.878536
[203]	training&#39;s auc: 0.951514	valid_1&#39;s auc: 0.878641
[204]	training&#39;s auc: 0.951658	valid_1&#39;s auc: 0.878818
[205]	training&#39;s auc: 0.95185	valid_1&#39;s auc: 0.878938
[206]	training&#39;s auc: 0.952031	valid_1&#39;s auc: 0.87908
[207]	training&#39;s auc: 0.952207	valid_1&#39;s auc: 0.879074
[208]	training&#39;s auc: 0.95235	valid_1&#39;s auc: 0.879052
[209]	training&#39;s auc: 0.952538	valid_1&#39;s auc: 0.879145
[210]	training&#39;s auc: 0.952727	valid_1&#39;s auc: 0.879258
[211]	training&#39;s auc: 0.952899	valid_1&#39;s auc: 0.879487
[212]	training&#39;s auc: 0.953072	valid_1&#39;s auc: 0.879544
[213]	training&#39;s auc: 0.953218	valid_1&#39;s auc: 0.879576
[214]	training&#39;s auc: 0.953421	valid_1&#39;s auc: 0.879667
[215]	training&#39;s auc: 0.953671	valid_1&#39;s auc: 0.879677
[216]	training&#39;s auc: 0.953902	valid_1&#39;s auc: 0.879695
[217]	training&#39;s auc: 0.954072	valid_1&#39;s auc: 0.879754
[218]	training&#39;s auc: 0.954332	valid_1&#39;s auc: 0.879895
[219]	training&#39;s auc: 0.954516	valid_1&#39;s auc: 0.879941
[220]	training&#39;s auc: 0.954662	valid_1&#39;s auc: 0.879992
[221]	training&#39;s auc: 0.954848	valid_1&#39;s auc: 0.880053
[222]	training&#39;s auc: 0.955008	valid_1&#39;s auc: 0.880176
[223]	training&#39;s auc: 0.955212	valid_1&#39;s auc: 0.880303
[224]	training&#39;s auc: 0.955398	valid_1&#39;s auc: 0.880363
[225]	training&#39;s auc: 0.955619	valid_1&#39;s auc: 0.880403
[226]	training&#39;s auc: 0.955831	valid_1&#39;s auc: 0.88054
[227]	training&#39;s auc: 0.956004	valid_1&#39;s auc: 0.880551
[228]	training&#39;s auc: 0.956167	valid_1&#39;s auc: 0.880625
[229]	training&#39;s auc: 0.956396	valid_1&#39;s auc: 0.88064
[230]	training&#39;s auc: 0.956552	valid_1&#39;s auc: 0.880748
[231]	training&#39;s auc: 0.956736	valid_1&#39;s auc: 0.880797
[232]	training&#39;s auc: 0.956833	valid_1&#39;s auc: 0.880889
[233]	training&#39;s auc: 0.957034	valid_1&#39;s auc: 0.880957
[234]	training&#39;s auc: 0.957195	valid_1&#39;s auc: 0.880963
[235]	training&#39;s auc: 0.957368	valid_1&#39;s auc: 0.881015
[236]	training&#39;s auc: 0.957497	valid_1&#39;s auc: 0.881099
[237]	training&#39;s auc: 0.957683	valid_1&#39;s auc: 0.881122
[238]	training&#39;s auc: 0.957858	valid_1&#39;s auc: 0.881114
[239]	training&#39;s auc: 0.95802	valid_1&#39;s auc: 0.881125
[240]	training&#39;s auc: 0.958191	valid_1&#39;s auc: 0.881147
[241]	training&#39;s auc: 0.958348	valid_1&#39;s auc: 0.881191
[242]	training&#39;s auc: 0.958525	valid_1&#39;s auc: 0.881227
[243]	training&#39;s auc: 0.95868	valid_1&#39;s auc: 0.881188
[244]	training&#39;s auc: 0.958837	valid_1&#39;s auc: 0.881239
[245]	training&#39;s auc: 0.958998	valid_1&#39;s auc: 0.881294
[246]	training&#39;s auc: 0.959182	valid_1&#39;s auc: 0.88135
[247]	training&#39;s auc: 0.959329	valid_1&#39;s auc: 0.881492
[248]	training&#39;s auc: 0.959489	valid_1&#39;s auc: 0.881503
[249]	training&#39;s auc: 0.959633	valid_1&#39;s auc: 0.881541
[250]	training&#39;s auc: 0.959796	valid_1&#39;s auc: 0.881688
[251]	training&#39;s auc: 0.959936	valid_1&#39;s auc: 0.881683
[252]	training&#39;s auc: 0.960094	valid_1&#39;s auc: 0.881732
[253]	training&#39;s auc: 0.960271	valid_1&#39;s auc: 0.88178
[254]	training&#39;s auc: 0.960463	valid_1&#39;s auc: 0.881813
[255]	training&#39;s auc: 0.960683	valid_1&#39;s auc: 0.881925
[256]	training&#39;s auc: 0.960833	valid_1&#39;s auc: 0.881909
[257]	training&#39;s auc: 0.960949	valid_1&#39;s auc: 0.881994
[258]	training&#39;s auc: 0.961042	valid_1&#39;s auc: 0.882144
[259]	training&#39;s auc: 0.961181	valid_1&#39;s auc: 0.882217
[260]	training&#39;s auc: 0.961317	valid_1&#39;s auc: 0.882223
[261]	training&#39;s auc: 0.96148	valid_1&#39;s auc: 0.882277
[262]	training&#39;s auc: 0.961642	valid_1&#39;s auc: 0.882287
[263]	training&#39;s auc: 0.961798	valid_1&#39;s auc: 0.882307
[264]	training&#39;s auc: 0.961965	valid_1&#39;s auc: 0.882376
[265]	training&#39;s auc: 0.962118	valid_1&#39;s auc: 0.882376
[266]	training&#39;s auc: 0.962307	valid_1&#39;s auc: 0.882354
[267]	training&#39;s auc: 0.962467	valid_1&#39;s auc: 0.882366
[268]	training&#39;s auc: 0.962617	valid_1&#39;s auc: 0.882357
[269]	training&#39;s auc: 0.962765	valid_1&#39;s auc: 0.882415
[270]	training&#39;s auc: 0.962866	valid_1&#39;s auc: 0.882549
[271]	training&#39;s auc: 0.962967	valid_1&#39;s auc: 0.88266
[272]	training&#39;s auc: 0.963077	valid_1&#39;s auc: 0.882673
[273]	training&#39;s auc: 0.963176	valid_1&#39;s auc: 0.88269
[274]	training&#39;s auc: 0.963314	valid_1&#39;s auc: 0.882795
[275]	training&#39;s auc: 0.963441	valid_1&#39;s auc: 0.882826
[276]	training&#39;s auc: 0.963564	valid_1&#39;s auc: 0.882794
[277]	training&#39;s auc: 0.963721	valid_1&#39;s auc: 0.882895
[278]	training&#39;s auc: 0.96381	valid_1&#39;s auc: 0.88286
[279]	training&#39;s auc: 0.963945	valid_1&#39;s auc: 0.882937
[280]	training&#39;s auc: 0.964077	valid_1&#39;s auc: 0.883046
[281]	training&#39;s auc: 0.964201	valid_1&#39;s auc: 0.883009
[282]	training&#39;s auc: 0.964366	valid_1&#39;s auc: 0.883029
[283]	training&#39;s auc: 0.964546	valid_1&#39;s auc: 0.88299
[284]	training&#39;s auc: 0.964702	valid_1&#39;s auc: 0.883034
[285]	training&#39;s auc: 0.964862	valid_1&#39;s auc: 0.883033
[286]	training&#39;s auc: 0.96499	valid_1&#39;s auc: 0.883077
[287]	training&#39;s auc: 0.965153	valid_1&#39;s auc: 0.883055
[288]	training&#39;s auc: 0.96523	valid_1&#39;s auc: 0.88305
[289]	training&#39;s auc: 0.965384	valid_1&#39;s auc: 0.88309
[290]	training&#39;s auc: 0.965496	valid_1&#39;s auc: 0.883105
[291]	training&#39;s auc: 0.965637	valid_1&#39;s auc: 0.883092
[292]	training&#39;s auc: 0.965775	valid_1&#39;s auc: 0.883149
[293]	training&#39;s auc: 0.965926	valid_1&#39;s auc: 0.883232
[294]	training&#39;s auc: 0.966057	valid_1&#39;s auc: 0.883324
[295]	training&#39;s auc: 0.966165	valid_1&#39;s auc: 0.883349
[296]	training&#39;s auc: 0.966278	valid_1&#39;s auc: 0.883368
[297]	training&#39;s auc: 0.966426	valid_1&#39;s auc: 0.883476
[298]	training&#39;s auc: 0.96656	valid_1&#39;s auc: 0.883512
[299]	training&#39;s auc: 0.966679	valid_1&#39;s auc: 0.883537
[300]	training&#39;s auc: 0.966809	valid_1&#39;s auc: 0.883536
[301]	training&#39;s auc: 0.966968	valid_1&#39;s auc: 0.883626
[302]	training&#39;s auc: 0.967079	valid_1&#39;s auc: 0.883623
[303]	training&#39;s auc: 0.967177	valid_1&#39;s auc: 0.883701
[304]	training&#39;s auc: 0.967317	valid_1&#39;s auc: 0.883659
[305]	training&#39;s auc: 0.967482	valid_1&#39;s auc: 0.883694
[306]	training&#39;s auc: 0.967609	valid_1&#39;s auc: 0.883718
[307]	training&#39;s auc: 0.967774	valid_1&#39;s auc: 0.88375
[308]	training&#39;s auc: 0.967883	valid_1&#39;s auc: 0.883793
[309]	training&#39;s auc: 0.967987	valid_1&#39;s auc: 0.883805
[310]	training&#39;s auc: 0.968108	valid_1&#39;s auc: 0.883823
[311]	training&#39;s auc: 0.968261	valid_1&#39;s auc: 0.8838
[312]	training&#39;s auc: 0.968418	valid_1&#39;s auc: 0.883843
[313]	training&#39;s auc: 0.968544	valid_1&#39;s auc: 0.883846
[314]	training&#39;s auc: 0.968661	valid_1&#39;s auc: 0.883837
[315]	training&#39;s auc: 0.968794	valid_1&#39;s auc: 0.883863
[316]	training&#39;s auc: 0.96892	valid_1&#39;s auc: 0.883869
[317]	training&#39;s auc: 0.969076	valid_1&#39;s auc: 0.883834
[318]	training&#39;s auc: 0.96919	valid_1&#39;s auc: 0.883841
[319]	training&#39;s auc: 0.969382	valid_1&#39;s auc: 0.88381
[320]	training&#39;s auc: 0.969521	valid_1&#39;s auc: 0.883771
[321]	training&#39;s auc: 0.969655	valid_1&#39;s auc: 0.883847
[322]	training&#39;s auc: 0.969738	valid_1&#39;s auc: 0.88384
[323]	training&#39;s auc: 0.969871	valid_1&#39;s auc: 0.883835
[324]	training&#39;s auc: 0.969988	valid_1&#39;s auc: 0.883789
[325]	training&#39;s auc: 0.970095	valid_1&#39;s auc: 0.883866
[326]	training&#39;s auc: 0.97024	valid_1&#39;s auc: 0.883825
[327]	training&#39;s auc: 0.970345	valid_1&#39;s auc: 0.883824
[328]	training&#39;s auc: 0.970434	valid_1&#39;s auc: 0.883775
[329]	training&#39;s auc: 0.970535	valid_1&#39;s auc: 0.883833
[330]	training&#39;s auc: 0.970632	valid_1&#39;s auc: 0.883818
[331]	training&#39;s auc: 0.970755	valid_1&#39;s auc: 0.883778
[332]	training&#39;s auc: 0.970869	valid_1&#39;s auc: 0.883816
[333]	training&#39;s auc: 0.97099	valid_1&#39;s auc: 0.883884
[334]	training&#39;s auc: 0.971155	valid_1&#39;s auc: 0.883902
[335]	training&#39;s auc: 0.97125	valid_1&#39;s auc: 0.883838
[336]	training&#39;s auc: 0.971375	valid_1&#39;s auc: 0.883856
[337]	training&#39;s auc: 0.971468	valid_1&#39;s auc: 0.883816
[338]	training&#39;s auc: 0.971566	valid_1&#39;s auc: 0.883764
[339]	training&#39;s auc: 0.971665	valid_1&#39;s auc: 0.883811
[340]	training&#39;s auc: 0.971787	valid_1&#39;s auc: 0.88385
[341]	training&#39;s auc: 0.971912	valid_1&#39;s auc: 0.883892
[342]	training&#39;s auc: 0.972016	valid_1&#39;s auc: 0.883942
[343]	training&#39;s auc: 0.972138	valid_1&#39;s auc: 0.883933
[344]	training&#39;s auc: 0.972298	valid_1&#39;s auc: 0.883935
[345]	training&#39;s auc: 0.972394	valid_1&#39;s auc: 0.883972
[346]	training&#39;s auc: 0.972512	valid_1&#39;s auc: 0.883975
[347]	training&#39;s auc: 0.972668	valid_1&#39;s auc: 0.883999
[348]	training&#39;s auc: 0.972774	valid_1&#39;s auc: 0.883997
[349]	training&#39;s auc: 0.972884	valid_1&#39;s auc: 0.884065
[350]	training&#39;s auc: 0.972961	valid_1&#39;s auc: 0.88412
[351]	training&#39;s auc: 0.973085	valid_1&#39;s auc: 0.88421
[352]	training&#39;s auc: 0.973181	valid_1&#39;s auc: 0.884237
[353]	training&#39;s auc: 0.973274	valid_1&#39;s auc: 0.884226
[354]	training&#39;s auc: 0.973389	valid_1&#39;s auc: 0.884232
[355]	training&#39;s auc: 0.973538	valid_1&#39;s auc: 0.884201
[356]	training&#39;s auc: 0.973668	valid_1&#39;s auc: 0.884201
[357]	training&#39;s auc: 0.9738	valid_1&#39;s auc: 0.884197
[358]	training&#39;s auc: 0.973905	valid_1&#39;s auc: 0.884226
[359]	training&#39;s auc: 0.974004	valid_1&#39;s auc: 0.884225
[360]	training&#39;s auc: 0.974099	valid_1&#39;s auc: 0.884219
[361]	training&#39;s auc: 0.974215	valid_1&#39;s auc: 0.884201
[362]	training&#39;s auc: 0.974334	valid_1&#39;s auc: 0.884169
[363]	training&#39;s auc: 0.974426	valid_1&#39;s auc: 0.88419
[364]	training&#39;s auc: 0.974525	valid_1&#39;s auc: 0.884158
[365]	training&#39;s auc: 0.974588	valid_1&#39;s auc: 0.884137
[366]	training&#39;s auc: 0.974671	valid_1&#39;s auc: 0.884119
[367]	training&#39;s auc: 0.974805	valid_1&#39;s auc: 0.88414
[368]	training&#39;s auc: 0.974914	valid_1&#39;s auc: 0.884181
[369]	training&#39;s auc: 0.975053	valid_1&#39;s auc: 0.884161
[370]	training&#39;s auc: 0.975196	valid_1&#39;s auc: 0.88414
[371]	training&#39;s auc: 0.9753	valid_1&#39;s auc: 0.884104
[372]	training&#39;s auc: 0.975425	valid_1&#39;s auc: 0.884161
[373]	training&#39;s auc: 0.975573	valid_1&#39;s auc: 0.884118
[374]	training&#39;s auc: 0.97569	valid_1&#39;s auc: 0.884106
[375]	training&#39;s auc: 0.975822	valid_1&#39;s auc: 0.884127
[376]	training&#39;s auc: 0.975913	valid_1&#39;s auc: 0.884169
[377]	training&#39;s auc: 0.976001	valid_1&#39;s auc: 0.884169
[378]	training&#39;s auc: 0.976133	valid_1&#39;s auc: 0.884189
[379]	training&#39;s auc: 0.97625	valid_1&#39;s auc: 0.884167
[380]	training&#39;s auc: 0.976381	valid_1&#39;s auc: 0.884195
[381]	training&#39;s auc: 0.97654	valid_1&#39;s auc: 0.884305
[382]	training&#39;s auc: 0.976699	valid_1&#39;s auc: 0.884309
[383]	training&#39;s auc: 0.976815	valid_1&#39;s auc: 0.884301
[384]	training&#39;s auc: 0.976906	valid_1&#39;s auc: 0.884283
[385]	training&#39;s auc: 0.977021	valid_1&#39;s auc: 0.884233
[386]	training&#39;s auc: 0.977155	valid_1&#39;s auc: 0.884203
[387]	training&#39;s auc: 0.977231	valid_1&#39;s auc: 0.884172
[388]	training&#39;s auc: 0.977312	valid_1&#39;s auc: 0.884207
[389]	training&#39;s auc: 0.977423	valid_1&#39;s auc: 0.884185
[390]	training&#39;s auc: 0.977509	valid_1&#39;s auc: 0.884191
[391]	training&#39;s auc: 0.97759	valid_1&#39;s auc: 0.884208
[392]	training&#39;s auc: 0.977663	valid_1&#39;s auc: 0.884263
[393]	training&#39;s auc: 0.977764	valid_1&#39;s auc: 0.884278
[394]	training&#39;s auc: 0.977868	valid_1&#39;s auc: 0.884266
[395]	training&#39;s auc: 0.977991	valid_1&#39;s auc: 0.884295
[396]	training&#39;s auc: 0.978104	valid_1&#39;s auc: 0.884325
[397]	training&#39;s auc: 0.978179	valid_1&#39;s auc: 0.884374
[398]	training&#39;s auc: 0.978301	valid_1&#39;s auc: 0.884441
[399]	training&#39;s auc: 0.978392	valid_1&#39;s auc: 0.884411
[400]	training&#39;s auc: 0.978484	valid_1&#39;s auc: 0.8844
[401]	training&#39;s auc: 0.978622	valid_1&#39;s auc: 0.884413
[402]	training&#39;s auc: 0.97873	valid_1&#39;s auc: 0.884467
[403]	training&#39;s auc: 0.978817	valid_1&#39;s auc: 0.88444
[404]	training&#39;s auc: 0.978878	valid_1&#39;s auc: 0.884432
[405]	training&#39;s auc: 0.978958	valid_1&#39;s auc: 0.88448
[406]	training&#39;s auc: 0.979089	valid_1&#39;s auc: 0.884477
[407]	training&#39;s auc: 0.979158	valid_1&#39;s auc: 0.884521
[408]	training&#39;s auc: 0.979248	valid_1&#39;s auc: 0.884526
[409]	training&#39;s auc: 0.979359	valid_1&#39;s auc: 0.88451
[410]	training&#39;s auc: 0.979459	valid_1&#39;s auc: 0.884539
[411]	training&#39;s auc: 0.979567	valid_1&#39;s auc: 0.884627
[412]	training&#39;s auc: 0.979653	valid_1&#39;s auc: 0.884644
[413]	training&#39;s auc: 0.979729	valid_1&#39;s auc: 0.884658
[414]	training&#39;s auc: 0.979798	valid_1&#39;s auc: 0.884686
[415]	training&#39;s auc: 0.979887	valid_1&#39;s auc: 0.884705
[416]	training&#39;s auc: 0.979965	valid_1&#39;s auc: 0.884678
[417]	training&#39;s auc: 0.980046	valid_1&#39;s auc: 0.884715
[418]	training&#39;s auc: 0.980145	valid_1&#39;s auc: 0.884669
[419]	training&#39;s auc: 0.980234	valid_1&#39;s auc: 0.884725
[420]	training&#39;s auc: 0.980358	valid_1&#39;s auc: 0.884715
[421]	training&#39;s auc: 0.98043	valid_1&#39;s auc: 0.884721
[422]	training&#39;s auc: 0.980513	valid_1&#39;s auc: 0.884718
[423]	training&#39;s auc: 0.980624	valid_1&#39;s auc: 0.884693
[424]	training&#39;s auc: 0.98067	valid_1&#39;s auc: 0.884672
[425]	training&#39;s auc: 0.980774	valid_1&#39;s auc: 0.884632
[426]	training&#39;s auc: 0.980844	valid_1&#39;s auc: 0.884666
[427]	training&#39;s auc: 0.980923	valid_1&#39;s auc: 0.884722
[428]	training&#39;s auc: 0.981034	valid_1&#39;s auc: 0.884718
[429]	training&#39;s auc: 0.98112	valid_1&#39;s auc: 0.884723
[430]	training&#39;s auc: 0.981204	valid_1&#39;s auc: 0.884666
[431]	training&#39;s auc: 0.981306	valid_1&#39;s auc: 0.884646
[432]	training&#39;s auc: 0.98138	valid_1&#39;s auc: 0.88461
[433]	training&#39;s auc: 0.981454	valid_1&#39;s auc: 0.884636
[434]	training&#39;s auc: 0.981547	valid_1&#39;s auc: 0.884627
[435]	training&#39;s auc: 0.981628	valid_1&#39;s auc: 0.884619
[436]	training&#39;s auc: 0.981696	valid_1&#39;s auc: 0.884748
[437]	training&#39;s auc: 0.981759	valid_1&#39;s auc: 0.884736
[438]	training&#39;s auc: 0.981862	valid_1&#39;s auc: 0.884725
[439]	training&#39;s auc: 0.981908	valid_1&#39;s auc: 0.884736
[440]	training&#39;s auc: 0.981987	valid_1&#39;s auc: 0.884698
[441]	training&#39;s auc: 0.982118	valid_1&#39;s auc: 0.884696
[442]	training&#39;s auc: 0.982214	valid_1&#39;s auc: 0.884714
[443]	training&#39;s auc: 0.982289	valid_1&#39;s auc: 0.884699
[444]	training&#39;s auc: 0.982347	valid_1&#39;s auc: 0.884705
[445]	training&#39;s auc: 0.982433	valid_1&#39;s auc: 0.884713
[446]	training&#39;s auc: 0.982498	valid_1&#39;s auc: 0.884699
[447]	training&#39;s auc: 0.982572	valid_1&#39;s auc: 0.884726
[448]	training&#39;s auc: 0.982648	valid_1&#39;s auc: 0.884696
[449]	training&#39;s auc: 0.982705	valid_1&#39;s auc: 0.884719
[450]	training&#39;s auc: 0.982802	valid_1&#39;s auc: 0.884689
[451]	training&#39;s auc: 0.982864	valid_1&#39;s auc: 0.884676
[452]	training&#39;s auc: 0.982974	valid_1&#39;s auc: 0.884648
[453]	training&#39;s auc: 0.983046	valid_1&#39;s auc: 0.884686
[454]	training&#39;s auc: 0.983117	valid_1&#39;s auc: 0.884679
[455]	training&#39;s auc: 0.983201	valid_1&#39;s auc: 0.884668
[456]	training&#39;s auc: 0.983266	valid_1&#39;s auc: 0.884682
[457]	training&#39;s auc: 0.983332	valid_1&#39;s auc: 0.884718
[458]	training&#39;s auc: 0.983395	valid_1&#39;s auc: 0.884726
[459]	training&#39;s auc: 0.983498	valid_1&#39;s auc: 0.884668
[460]	training&#39;s auc: 0.983571	valid_1&#39;s auc: 0.8847
[461]	training&#39;s auc: 0.983636	valid_1&#39;s auc: 0.884647
[462]	training&#39;s auc: 0.983699	valid_1&#39;s auc: 0.884641
[463]	training&#39;s auc: 0.983767	valid_1&#39;s auc: 0.884624
[464]	training&#39;s auc: 0.983838	valid_1&#39;s auc: 0.884598
[465]	training&#39;s auc: 0.983928	valid_1&#39;s auc: 0.884619
[466]	training&#39;s auc: 0.984016	valid_1&#39;s auc: 0.884595
[467]	training&#39;s auc: 0.98409	valid_1&#39;s auc: 0.884613
[468]	training&#39;s auc: 0.984172	valid_1&#39;s auc: 0.884573
[469]	training&#39;s auc: 0.98424	valid_1&#39;s auc: 0.884624
[470]	training&#39;s auc: 0.984319	valid_1&#39;s auc: 0.884651
[471]	training&#39;s auc: 0.984386	valid_1&#39;s auc: 0.884678
[472]	training&#39;s auc: 0.98447	valid_1&#39;s auc: 0.884631
[473]	training&#39;s auc: 0.984547	valid_1&#39;s auc: 0.884661
[474]	training&#39;s auc: 0.984601	valid_1&#39;s auc: 0.884673
[475]	training&#39;s auc: 0.984682	valid_1&#39;s auc: 0.88464
[476]	training&#39;s auc: 0.984763	valid_1&#39;s auc: 0.884595
[477]	training&#39;s auc: 0.984849	valid_1&#39;s auc: 0.884604
[478]	training&#39;s auc: 0.984964	valid_1&#39;s auc: 0.884576
[479]	training&#39;s auc: 0.985012	valid_1&#39;s auc: 0.8846
[480]	training&#39;s auc: 0.985118	valid_1&#39;s auc: 0.884615
[481]	training&#39;s auc: 0.985209	valid_1&#39;s auc: 0.884617
[482]	training&#39;s auc: 0.985291	valid_1&#39;s auc: 0.884631
[483]	training&#39;s auc: 0.985352	valid_1&#39;s auc: 0.884669
[484]	training&#39;s auc: 0.985433	valid_1&#39;s auc: 0.884643
[485]	training&#39;s auc: 0.98551	valid_1&#39;s auc: 0.884679
[486]	training&#39;s auc: 0.985593	valid_1&#39;s auc: 0.884688
[487]	training&#39;s auc: 0.985665	valid_1&#39;s auc: 0.88469
[488]	training&#39;s auc: 0.985733	valid_1&#39;s auc: 0.884678
[489]	training&#39;s auc: 0.985791	valid_1&#39;s auc: 0.884708
[490]	training&#39;s auc: 0.985853	valid_1&#39;s auc: 0.88468
[491]	training&#39;s auc: 0.985927	valid_1&#39;s auc: 0.884659
[492]	training&#39;s auc: 0.985991	valid_1&#39;s auc: 0.884626
[493]	training&#39;s auc: 0.986051	valid_1&#39;s auc: 0.884647
[494]	training&#39;s auc: 0.986127	valid_1&#39;s auc: 0.884638
[495]	training&#39;s auc: 0.986188	valid_1&#39;s auc: 0.884668
[496]	training&#39;s auc: 0.986247	valid_1&#39;s auc: 0.884695
[497]	training&#39;s auc: 0.986301	valid_1&#39;s auc: 0.884669
[498]	training&#39;s auc: 0.986402	valid_1&#39;s auc: 0.884671
[499]	training&#39;s auc: 0.986474	valid_1&#39;s auc: 0.884723
[500]	training&#39;s auc: 0.98653	valid_1&#39;s auc: 0.884682
[501]	training&#39;s auc: 0.986618	valid_1&#39;s auc: 0.884654
[502]	training&#39;s auc: 0.986668	valid_1&#39;s auc: 0.884627
[503]	training&#39;s auc: 0.986718	valid_1&#39;s auc: 0.884637
[504]	training&#39;s auc: 0.986758	valid_1&#39;s auc: 0.884639
[505]	training&#39;s auc: 0.986808	valid_1&#39;s auc: 0.884597
[506]	training&#39;s auc: 0.986874	valid_1&#39;s auc: 0.884613
[507]	training&#39;s auc: 0.986946	valid_1&#39;s auc: 0.884606
[508]	training&#39;s auc: 0.987007	valid_1&#39;s auc: 0.884672
[509]	training&#39;s auc: 0.987075	valid_1&#39;s auc: 0.884642
[510]	training&#39;s auc: 0.987147	valid_1&#39;s auc: 0.884592
[511]	training&#39;s auc: 0.987207	valid_1&#39;s auc: 0.884585
[512]	training&#39;s auc: 0.98727	valid_1&#39;s auc: 0.88459
[513]	training&#39;s auc: 0.987317	valid_1&#39;s auc: 0.884557
[514]	training&#39;s auc: 0.98737	valid_1&#39;s auc: 0.884545
[515]	training&#39;s auc: 0.987448	valid_1&#39;s auc: 0.884542
[516]	training&#39;s auc: 0.987505	valid_1&#39;s auc: 0.884542
[517]	training&#39;s auc: 0.98756	valid_1&#39;s auc: 0.884565
[518]	training&#39;s auc: 0.987624	valid_1&#39;s auc: 0.88457
[519]	training&#39;s auc: 0.987682	valid_1&#39;s auc: 0.884555
[520]	training&#39;s auc: 0.987759	valid_1&#39;s auc: 0.88458
[521]	training&#39;s auc: 0.98782	valid_1&#39;s auc: 0.884575
[522]	training&#39;s auc: 0.987866	valid_1&#39;s auc: 0.884551
[523]	training&#39;s auc: 0.987918	valid_1&#39;s auc: 0.884527
[524]	training&#39;s auc: 0.987963	valid_1&#39;s auc: 0.884506
[525]	training&#39;s auc: 0.988019	valid_1&#39;s auc: 0.884535
[526]	training&#39;s auc: 0.988057	valid_1&#39;s auc: 0.884516
[527]	training&#39;s auc: 0.988103	valid_1&#39;s auc: 0.884512
[528]	training&#39;s auc: 0.98816	valid_1&#39;s auc: 0.884481
[529]	training&#39;s auc: 0.988232	valid_1&#39;s auc: 0.884493
[530]	training&#39;s auc: 0.988272	valid_1&#39;s auc: 0.884549
[531]	training&#39;s auc: 0.988342	valid_1&#39;s auc: 0.884557
[532]	training&#39;s auc: 0.988418	valid_1&#39;s auc: 0.884577
[533]	training&#39;s auc: 0.988479	valid_1&#39;s auc: 0.884597
[534]	training&#39;s auc: 0.988553	valid_1&#39;s auc: 0.884574
[535]	training&#39;s auc: 0.988623	valid_1&#39;s auc: 0.884603
[536]	training&#39;s auc: 0.988662	valid_1&#39;s auc: 0.884601
Early stopping, best iteration is:
[436]	training&#39;s auc: 0.981696	valid_1&#39;s auc: 0.884748





LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=0.7,
        importance_type=&#39;split&#39;, learning_rate=0.1, max_depth=-1,
        metric=&#39;auc&#39;, min_child_samples=20, min_child_weight=0.001,
        min_split_gain=0.0, n_estimators=2500, n_jobs=-1, num_leaves=30,
        objective=&#39;binary&#39;, random_state=2018, reg_alpha=0, reg_lambda=0.0,
        silent=True, subsample=0.9, subsample_for_bin=200000,
        subsample_freq=1)
</code></pre>
<h1 id="预测结果"><a href="#预测结果" class="headerlink" title="预测结果"></a>预测结果</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict=lgb_model.predict_proba(test_x)[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = pd.DataFrame()</span><br><span class="line">result[<span class="string">&#x27;ID_code&#x27;</span>] = test[<span class="string">&#x27;ID_code&#x27;</span>]</span><br><span class="line">result[<span class="string">&#x27;target&#x27;</span>] = predict</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result.to_csv(<span class="string">&#x27;submission&#x27;</span>,index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<ol>
<li><ol>
<li>最后代码得分：0.88918<img src="/"></li>
</ol>
</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/06/30/%E5%A6%82%E4%BD%95%E6%9B%B4%E5%A5%BD%E5%9C%B0%E8%BF%9B%E8%A1%8COKR%E7%AE%A1%E7%90%86-%E4%B8%8B%E5%8D%887-56-23/" rel="prev" title="如何更好地进行OKR管理">
                  <i class="fa fa-chevron-left"></i> 如何更好地进行OKR管理
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/02/19/%E5%A6%82%E4%BD%95%E5%86%99%E5%87%BA%E4%B8%80%E4%BB%BD%E7%BB%93%E6%9E%84%E5%8C%96%E7%9A%84PRD/" rel="next" title="如何写出一份结构化的PRD">
                  如何写出一份结构化的PRD <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">一颗西蓝花🥦</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
